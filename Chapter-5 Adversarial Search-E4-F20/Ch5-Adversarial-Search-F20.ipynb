{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 471: Introduction to Artificial Intelligence</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpeg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Adversarial Search & \"Games\"</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Chapter 5: Adversarial Search</center></h1>\n",
    "\n",
    "<h5><center>Also Known as Games</center></h5>\n",
    "\n",
    "<h7><center>\"In which we examine the problems that arise when we try to plan ahead in a world where other agents are planning against us.\"</center></h7>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Assumptions: Zero-Sum and Perfect Information</center></h1>\n",
    "\n",
    "- Initially, we focus on games that are deterministic and completely observable.\n",
    "\n",
    "- We also assume that the payoff to each player at the end of a game is equal and opposite, called **zero-sum**.\n",
    "\n",
    "- Moreover, the two players both have access to complete information about the states that may lead to a better move and eventually win/loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Zero-Sum</center></h1>\n",
    "\n",
    "- Imagine a win is value 1, a loss is value 0, and a draw is 1/2.\n",
    "\n",
    "\n",
    "|  |   Result   |   Subtract 1/2  |\n",
    "|  :--:  |  :--:  |  :--:  |\n",
    "|  Win,Loss   |  Player A = 1,  Player B = 0  |  Player A = 1/2  Player B = -1/2  |\n",
    "|   Draw  |  Player A = 1/2,  Player B = 1/2  |  Player A = 0, Player B = 0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Definition of Game</center></h1>\n",
    "\n",
    "  * initial state $s_0$\n",
    "  * $player(s)$: which player is to move in state $s$,\n",
    "  * $actions(s)$: legal actions from state $s$,\n",
    "  * $result(s,a)$: state that results\n",
    "  * $terminalTest(s)$: true when game is over\n",
    "  * $utility(s,p)$: payoff for player $p$ upon reaching state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>MiniMax</center></h1>\n",
    "\n",
    "The two players in a two person game will be called `Max` and\n",
    "`Min`. These names reflect the meaning of the $utility(s,p)$ \n",
    "function, which is to be maximized by Player `Max` and minimized by\n",
    "Player `Min`. \n",
    "\n",
    "The partial search tree in the next slide illustrates the\n",
    "reasoning behind the concept of alternate layers minimizing and\n",
    "maximizing the utility value to back up a value from terminal states\n",
    "to non-terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>MiniMax Example</center></h1>\n",
    "\n",
    "<center><img src=\"img/fig-5-2.png\" align=\"center\"/></center>\n",
    "\n",
    "From Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>MiniMax</center></h1>\n",
    "\n",
    "The calculation of the `minimax(s)` value of a state $s$ can be\n",
    "summarized as\n",
    "\n",
    "$$\n",
    "\\text{minimax}(s) = \\begin{cases}\n",
    "utility(s), & \\text{if }terminalTest(s);\\\\\n",
    "\\max_{a\\in actions(s)} \\text{minimax}(result(s,a)), & \\text{if\n",
    "}player(s) \\text{ is Max};\\\\\n",
    "\\min_{a\\in actions(s)} \\text{minimax}(result(s,a)), & \\text{if\n",
    "}player(s) \\text{ is Min}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Assumes player `Min` plays optimally.  If not, `Max` will do even\n",
    "better.\n",
    "\n",
    "The textbook shows in Figure 5.3 the *minimax-decision* algorithm as\n",
    "a depth-first search that altenates between calling `max-value` and\n",
    "`min-value` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>MiniMax Tic-Tac-Toe</center></h1>\n",
    "\n",
    "<center><img src=\"img/fig-5-1.png\" align=\"center\"/></center>\n",
    "\n",
    "From Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"minmax.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f3bf40cd898>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"minmax.pdf\", width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Alpha-Beta Pruning</center></h1>\n",
    "\n",
    "Some of the search tree can be ignored if we know we cannot find a\n",
    "better move from the best one found so far.  If you are Player X in\n",
    "Tic-Tac-Toe, and\n",
    "  * your best move so far will result in a draw, and\n",
    "  * the next move you are evaluating you discover your opponent can definitely win from,\n",
    "  * do not explore any other choices your opponent might have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Alpha-Beta Pruning</center></h1>\n",
    "\n",
    "For each node, keep track of three values, Minimax value (the same value returned by Minimax algorithm), as well as $\\alpha$ and $\\beta$\n",
    "\n",
    "$\\alpha$ is best value by any means\n",
    "  * Any value less than this is no use because we already now how to achieve at least a value of $\\alpha$\n",
    "  * $\\alpha = Max(current value, new value)$\n",
    "  * Initially, $- \\infty$\n",
    "  * $\\alpha$ is updated only at Max nodes\n",
    "\n",
    "$\\beta$ is worst value for the opponent\n",
    "  * $\\beta = Min(current value, new value)$\n",
    "  * Initially, $+ \\infty$\n",
    "  * $\\beta$ is updated only at Min nodes\n",
    "  \n",
    "The span between $\\alpha$ and $\\beta$ progressively gets smaller.\n",
    "\n",
    "Any unvisited children/subtree of the node for which $\\beta <= \\alpha$ can be pruned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"alpha-beta-example.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f3bf40cde48>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"alpha-beta-example.pdf\", width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Alpha-Beta Pruning - Practice</center></h1>\n",
    "\n",
    "[Alpha-Beta Practice](abTreePractice-master/index.html) --- Link is local! Please refer to the class activities that were shared in Piazza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Evaluation Functions</center></h1>\n",
    "\n",
    "- Used to evaluate the \"goodness\" of a game position\n",
    "\n",
    "- Contrast with heuristic search, where evaluation function is an estimate of cost from start node to goal passing through given node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Evaluation Functions</center></h1>\n",
    "\n",
    "- $f(n) >> 0$ position n good for me; bad for you\n",
    "\n",
    "- $f(n) << 0$ position n good for you; bad for me\n",
    "\n",
    "- $f(n)$ near $0$ position n is a neutral position\n",
    "\n",
    "- $f(n) = +\\infty$ position n win for me\n",
    "\n",
    "- $f(n) = -\\infty$ position n win for you\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Evaluation Function Example - Chess</center></h1>\n",
    "\n",
    "- Claude Shannon's paper *Programming a Computer for Playing Chess (1950)* was among the first proposals to apply evaluation functions to states in the search.\n",
    "\n",
    "\n",
    "- Alan Turing’s function for chess:\n",
    "- $f(n) = w(n) / b(n)$ where $w(n)$ is sum of point value of white’s pieces and $b(n)$ is black's pieces.\n",
    "\n",
    "\n",
    "- Traditional piece values: pawn:1; knight:3; bishop:3; rook:5; queen:9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Evaluation Function Formulation</center></h1>\n",
    "\n",
    "$$Eval(s) = w_1f_1(s) + w_2f_2(s) + ... w_nf_n(s) = \\sum_{i=1}^n w_i f_i$$\n",
    "\n",
    "where each $w_i$ is a weight and each $f_i$ is a feature of the position. For chess, $f_i$ could be the numbers of each kind of piece on the board and the $w_i$ could be the values of the pieces.\n",
    "\n",
    "- IBM’s chess program [Deep Blue](https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)) (circa 1996) had $>8K$ features in its evaluation function.\n",
    "\n",
    "- In DeepBlue’s alpha-beta pruning, average branching factor at node was ~6 instead of ~35! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>IBM Deep Blue</center></h1>\n",
    "\n",
    "<center><img src=\"img/deepblue.jpg\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Evaluation Function Estimation</center></h1>\n",
    "\n",
    "- In games where not so much experience is available like chess, the weights of the evaluation function can be estimated by the machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Limitations of Alpha-Beta Pruning</center></h1>\n",
    "\n",
    "- Despite the strength of Alpha-Beta pruning in search tree reduction, it has two major weaknesses:\n",
    "\n",
    "    - First, if branching factor of tha game tree is too high (say 361 in Go game), alpha-beta would be limited to only 4 or 5 ply.\n",
    "\n",
    "    - Second, it is difficult to define a good evaluation function for some games like Go.\n",
    "    \n",
    "\n",
    "- In response to these challenges, some modern game programs have abandoned alpha-beta search and instead use a strategy called **Monte Carlo Tree Search MCTS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Monte Carlo Tree Search MCTS</center></h1>\n",
    "\n",
    "- The basic MCTS strategy does not use a heuristic function evaluation. Instead, the value of a state is estimated as the average utility over a number of **simulations** of complete games starting from the state.\n",
    "\n",
    "\n",
    "- A **simulation** (also called a **playout** or **rollout**) chooses moves first for one player, then for the other, repeating until a terminal position is reached. At that point, the rules of the game (not fallible heuristics) determine who has won or lost and by what score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Exploration-Exploitation Tradeoff in MCTS</center></h1>\n",
    "\n",
    "- Exploration-Exploitation tradeoff in MCTS is done iteratively through four steps:\n",
    "    - Selection\n",
    "    - Expansion\n",
    "    - Simulation\n",
    "    - Backpropagation\n",
    "\n",
    "<center><img src=\"img/fig-5-10.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Stochastic Games</center></h1>\n",
    "\n",
    "- A stochastic game is modeled by simply adding a level of **chance nodes** between each player's levels in the search tree.\n",
    "\n",
    "<center><img src=\"img/fig-5-13.png\" align=\"center\"/></center>\n",
    "\n",
    "Image from Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Stochastic Games Example</center></h1>\n",
    "\n",
    "First, a definition of *expected value*.  The average value of a lot\n",
    "(infinite number) of dice rolls with a fair dice is\n",
    "\n",
    "$$\n",
    "(1+2+3+4+5+6) / 6\n",
    "$$\n",
    "\n",
    "The *expected value* is exactly this average, but is defined as the\n",
    "sum of the possible values times their probability of occurring.\n",
    "\n",
    "$$\n",
    "1(1/6) + 2(1/6) + 3(1/6) + 4(1/6) + 5(1/6) + 6(1/6)\n",
    "$$\n",
    "\n",
    "If the 4, 5 and 6 sides are less likely than the other sides, then the\n",
    "expected value might be\n",
    "\n",
    "$$\n",
    "1(1/4) + 2(1/4) + 3(1/4) + 4(1/12) + 5(1/12) + 6(1/12)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Stochastic Games - Expectiminimax Values</center></h1>\n",
    "\n",
    "- The various outcomes from the chance node have certain probabilities of occurring.  When backing up values through a chance node, the values are multiplied by their probability of occurring.\n",
    "\n",
    "\n",
    "- This illustrates  the **expectiminimax** values, for backing up values through chance nodes.\n",
    "\n",
    "<center><img src=\"img/expectedvalues.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from Professor Chuck Anderson's Notebooks - CSU</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Limitations of Game Search Algorithms</center></h1>\n",
    "\n",
    "- One limitation of Alpha-beta is its vulnerability to errors in the heuristic function.\n",
    "\n",
    "\n",
    "- A second limitation for both Alpha-beta and MCTS is that they are designed to calculate the values of legal moves. But sometimes there is one move that is obviously the best (only one legal move), and in that case, there is no point wasting computational time to figure out the value of the move.\n",
    "\n",
    "\n",
    "- A third limitation is that both alpha-beta and MCTS do all their reasoning at the level of individual moves. Clearly, humans play games differently: they can reason at a more abstract level, considering a higher-level goal---for example trapping the opponent's queen---and using the goal to selectively generate plausible plans.\n",
    "\n",
    "\n",
    "- A fourth issue is the ability to incorporate **Machine Learning** into the game search process. Early game programs relied on human expertise to hand-craft evaluation functions. Nowadays, more games rely on machine learning from self-play rather than game-specific human-generated expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Adversarial Search Summary</center></h1>\n",
    "\n",
    "- Games Assumptions: Zero-Sum and Perfect Information\n",
    "\n",
    "- Definition of Games\n",
    "\n",
    "- Minimax\n",
    "\n",
    "- Alpha-Beta Pruning\n",
    "\n",
    "- Evaluation Functions\n",
    "\n",
    "- Stochastic Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>Credit</center></h1>\n",
    "\n",
    "- Some texts of these slides are directly quoted from AIMA textbook 4th Edition by Russel and Norvig."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
